#+TITLE:Replay Detection

* TODO
replace Two-Stream Convolutional Networks for Action Recognition in Videos (etc) with reference like (ex [2])

* Introduction
** Problématique
Au cours des dernières années, les techniques d'apprentissage automatique ont joué un rôle de plus en plus important dans les systèmes de reconnaissance automatique.
Les avancements dans le domaine de l'apprentissage profond et l'accès à un volume massif de données ont permi de mettre en place des solutions de plus en plus performantes.
En particulier, l'adoption globale des téléphones intelligents et de leur caméra intégrée a accru de manière exponentielle le nombre de vidéos disponibles sur Internet,
à tel point qu'il est devenu impossible pour l'humain d'ingérer manuellement le contenu de toutes les vidéos disponibles sur la toile.
Dans cet étude, nous allons nous intéressons à la tâche de "video summarization" ou, en français, de récapitulation de vidéo.
Plus particulièrement, nous focalisons notre recherche sur les vidéos de match de football.
Dans ces vidéos, notre objectif va être de parvenir à identifier les replays; car à partir de ces replays, nous serons capables de mettre en avant les moments importants du match.

* Pourquoi les replays ?*
Un replay est la retransmission d'une action qui s'est déjà passé au cours d'une vidéo.
Les replays sont intéressants car ils sont un indicateur d'un moment fort dans une vidéo.
En effet, c'est l'équipe technique chargée du montage de la vidéo qui décide ou non de créer un replay pour une action, un replay est une annotation humaine sur une vidéo.
Typiquement, un replay sera incrusté dans la vidéo après une action importante comme, par exemple, un but ou un pénaltie.

* Caractéristique des replays*
Les replays sont introduits et se terminent par un logo (Pan et al. *citation*).
Ces logos ont en général une apparence qui se démarque facilement des autres images dans la vidéo.
Les replays ne sont pas à confondre avec les ralentis.
Les ralentis sont un type particulier de replays où l'action est montrée de nouveau en /slow-motion/, mais tous les replays ne sont pas des ralentis.
C'est pourquoi, la vitesse de déplacement des objets dans l'image (pour détecter l'effet de /slow-motion/) n'est pas un bon critère pour la détection de replay.


** Objectif de la recherche
L'objectif de notre recherche est double.
Dans un premier temps, nous voulons mettre en place un système de détection de replays en utilisant uniquement les méthodes "classiques" tels que ORB (*ref*)
Nous voulons comparer l'efficacité des méthodes de vision par ordinateur "classique" aux méthodes par apprentissage profond (deep learning).
L'objectif dans cette parti est double. Le premier est de détecter le plus efficacement les logos dans les vidéos afin d'avoir une *base line* à comparer avec l'approche par deep learning.
Le second est de mettre en place un système capable d'extraire les frames avec des logo, et ce sans intervention humaine (pas de labeling, pas d'ensemble d'entraînement à construire), afin de pouvoir constituer automatiquement un ensemble de données permettant d'entraîner des modèles de machine learning.


* Première partir : Détection sans deep learning
* Détection des replays : êtat de l'art
** A Robust Replay Detection
Cette approche détecte les replays en trouvant les logos dans la vidéo.
Les logos sont trouvés grâce à la luminance. Nous savons qu'un logo est présent pendant 0.8 secondes soit 18 frames pour une vidéo de 24 FPS.
Tous les frames qui ont obtenu un score supérieur à un certain seuil sont considérés comme des "logo templates".

*** Calcul du score de la luminance et filtrage des frames
L'idée est de parcourir toute la vidéo et de calculer pour chaque frame la différence de luminance qu'il y a entre ce frame et les 17 frames précédents.
Nous obtenons un score L_i pour chaque frame i dans la vidéo.
Tous les frames dont le score est inférieur à un certain seuil sont écartés, les autres vont servir à trouver le logo template.

*** Recherche du logo template parmis les frames filtrés
Le logo template est le frame qui représente le mieux tous les logos dans la vidéo.
Pour déterminer le logo template parmis les frames filtrés, l'algorithme K-means est utilisé pour séparer cet ensemble en deux (K = 2) en fonction de la luminance moyenne des frames.
Pour trouver le logo template, nous allons chercher dans le cluster avec le centre de cluster le plus élevé, puis sélection le frame m minimisant la distance avec tous les autres frames du cluster.
[[file:robust_calc_dist.JPG]]
Source : Xu, W., & Yi, Y. (2011). A Robust Replay Detection Algorithm for Soccer Video. IEEE Signal Processing Letters, 18(9). Equation (4)

*** Recherche des logo
Une fois que le logo template est déterminé, chaque logo trouvé en 1 va être comparé avec le logo template.
La mesure de comparaison est la distance (équation 4) qu'il y a entre le frame et le template dans le cluster.
Tous les frames qui ont une distance inférieure à un certain threshold sont considérés comme des logos.

*** Recherche des replays
Une fois que les logos sont détectés, nous pouvons trouver les replays en cherchant les paires de logos éloignés de moins de 80 seconde (durée maximum d'un replay).

*** Résultats obtenus et conclusion sur cette méthode
[[file:robust_res.JPG]]
Après avoir implémenté cette méthode, nous avons constaté que celle-ci n'est pas efficace et ne fonctionne pas du tout sur notre ensemble de test.
Cette approche est trop dépendant de la luminance et elle ne parvient pas à détecter les logos peu lumineux.
De plus, les bases mathématiques (notamment la manière de choisir le cluster et la mesure de distance) sont un peu douteuses.
Enfin, celle-ci dépend trop du paramètre "seuil de luminance" affectant les logos détectés.
Le seuil de luminance fournis par les auteurs ne produit pas de bons résultats sur toutes les vidéos.
Nous n'avons pas réussi à trouver une valeur pour le seuil de luminance qui obtienne universellemnt de bons résultats.
Un seuil à 100000 détecte les logos de Ligue 1 mais pas les logos de Liga.
Un seuil de 75000 détecte les logos de Liga et de Ligue 1, mais laisse passer trop de faux positifs.
Les logos de Premier League quant à eux ne sont pas tous détectés avec un seuil à 50000, alors que ce seuil accepte un grand nombre de faux positifs.
Pour conclure, cette approche n'est pas celle qui va nous permettre de mettre en place un système de détection de replays robuste et efficace.

** Fast Highlight Detection and Scoring for Broadcast Soccer Video Summarization using On-Demand Feature Extraction and Fuzzy Inference
*** Idée
Entrainer un algorithme CART des histogrammes 3D sur des frames de logos choisi à la main.
Utilisé l'algorithme CART sur les histogrammes pour prédire logo/non-logo.

*** Conclusion
Nous ne voulons pas maintenir un ensemble d’apprentissage. Cette méthode ne convient pas à nos besoins.

** Automatic summarization of soccer highlights using audio-visual descriptors
*** Idée
S = Détecter tous les shots (plans) dans la vidéo
L = Pour chaque shot S_i:
- L_i_start = La "luminance" des frames au début du shot
- L_i_end = La "luminance" des frames à la fin du shot
- L_template = Trouver le "logo template" dans L
- Pour chaque logo l dans L:
    - Diff l avec L_template = conversion grayscale puis somme de la soustraction pixel par pixel
    - Si Diff l avec L_template < threshold => l est un logo

*** Conclusion
Cette méthode est trop semblable à l’approche “Robust Replay Detection” qui ne répond pas à nos besoin,
cette approche ne fonctionnera pas dans notre cas (la luminance n’est pas un critère assez discriminant
pour la reconnaissance de logo). Néanmoins, l’idée de découper la vidéo en “shot” (en plan) est
intéressante et nous nous en servons par la suite.

** MEAN SHIFT BASED VIDEO SEGMENT REPRESENTATION AND APPLICATIONS TO REPLAY DETECTION

*** Idée
Segmenter la vidéo en frame, puis calculer une représentation compressée de chaque frame.
Pour détecter les logo (ou n'importe quoi), il faut d'abord "apprendre" plusieurs formes
compressées de logo (sur des vidéos d'apprentissage que nous aurons labelisé à la main),
puis il faut simplement calculer une distance entre la forme compressé du shot à définir
et les formes compressées apprises.

*** Conclusion
Cette approche est intéressante, néanmoins l’article n’est pas assez précis, notamment
sur la manière dont les images sont compressées. De plus, nous ne voulons par maintenir
un ensemble d’apprentissage. Cette méthode ne convient donc pas à nos besoins.

** Real-time field sports scene classification using colour and frequency space decompositions
*** Idée
Classifie les shots en fonction de la distance (proche, moyen, loin) et de ce qu'il y a
dedans (visage, épaule, un seul joueur,  plusieurs joueurs, terrain, spectateur).

*** Conclusion
A l'air d'être une approche solide. Néanmoins, c'est de la classification supervisée,
il faut donc un ensemble d'apprentissage. Cette méthode ne convient donc pas à nos besoins.


* Les approches proposées
** TODO : mettre en avant le fait que l'algo va être mis en prod
Les méthodes "état de l'art" ne donnent pas d'assez bons résultats et ne peuvent
pas être mises en production. Nous cherchons donc notre propre méthode.

Pour détecter les replays, nous faisons les hypothèses que :
- un replay a un logo de début (I)
- un replay a un logo de fin (II)
- les logos de début et de fin sont les mêmes (III)
- les logos ont une forme facilement reconnaissable qui se distingue des  autres images dans la vidéo (IV)
- un replay dure entre 2 et 90 secondes (V)

Nous proposons plusieurs approches permettant de détecter les logo de replay dans
les vidéo de sport. Dans cette partie, chacune de ces approches n'utilisent que des algorithmes
de computer vision classique (flouttage, filtre de Canny, ORB, ...) et des algorithmes de machine
learning non-supervisés (K-NN).
Ces restrictions s'appliquent pour les raisons suivantes :
- le programme doit être le plus rapide possible (les réseaux de neurones sont en général trop lents,
  trop exigeant en ressource); d'où le choix d'algorithme plus simple.
- si la solution doit être mise en production, il est préférable de ne pas avoir d'ensemble d'apprentissage
  à obtenir ou maintenir; d'où le choix d'algorithme non-supervisé uniquement.

** Détection des plans
Les approches que nous proposons itérent sur tous les frames de la vidéo, à la recherche des
logo pouvant se trouver au début et à la fin des replays. Si nous faisons l'hypothèse qu'un
replay entraînera toujours un changement de plan, alors au lieu de rechercher les logo
parmi tous les frames de la vidéo, nous réduisons la recherche à tous les frames qui sont entre deux
plans.
*** ONLINE, SIMULTANEOUS SHOT BOUNDARY DETECTION AND KEY FRAME EXTRACTION FOR SPORTS VIDEOS USING RANK TRACING
Cette méthode est proposée par W. Abd-Almageed en 2008.

Chaque frame est converti en HSV et les histogrammes H, S et V sont calculés.
Un vecteur est formé pour chaque frame à partir de ces histogrammes.

Ensuite, une matrice M de dimension N * L, représentant une fenêtre de N frames va
être formée à partir de ces vecteurs, où L est la taille des histogrammes et N la taille de la fenêtre.

L'algorithme SVD (singular value decomposition) va être appliquée sur M. M = UWV,
où W est la matrice de valeur singulière.

Les diagonales de la matrice W comportent des poids S ordonnées de manière non croissante.
Le premier poid S_1 est le poid maximal. Ces poids représentent l'information contenu dans le vecteur V.

Nous allons assigner un rang à la matrice M,  ce rang va être égal au nombre d'élement s dans S
tel que s/S1 > threshold. Le rang va être calculé pour chaque fenêtre de frame dans la vidéo.

Si le rang d'une fenêtre est plus que grand que le rang de la fenêtre avant elle, alors le
contenu visuel de la fenêtre est différent de la fenêtre précédente.
A l'inverse, si le rang est inférieure à la fenêtre précédente,
alors le contenu visuel se stabilise. S'il est de 1, alors c'est stable.

Le début d'un frame est celui qui maximise le rang parmis les fenêtres environnantes.

**** Résultats obtenus et conclusion
Cette méthode pour trouver les plans dans une vidéo est très efficace, et constitue la
base de la suite de notre recherche.

En effet, avant de segmenter la vidéo en plan, nous comparions  N frames , où N peut être
aussi grand que 400000 (pour des vidéo de 120 minutes à 60 fps), il est impensable d’utiliser
un algorithme en O(N²), par exemple en comparant toutes les frames entre elles, avec un N aussi grand.

Après avoir segmenter la vidéo en plan, nous obtenons un N’ au alentours de 2000 pour une vidéo
de 120 minutes à 60 fps. Nous pouvons donc nous permettre d’utiliser des algorithmes plus
complexes que sans la segmentation en plan.
De plus, la segmentation en plan réduit le champs de recherche des frames logo,
et donc le nombre de faux positifs potentiels.


* Première approche : ORB
Dans cette approche, nous cherchons à reconnaître les logo dans la vidéo.
Pour ce faire, nous optons pour une approche de clustering. L'idée
est de clusteriser la vidéo en deux groupe : un groupe pour les frames
logo, et un autre groupe pour les frames non-logo.

** Extraction des caractéristiques
OpenCV permet d'extraire des features à partir des images (détection des bords
des objets dans l'image).
A partir de ça, nous pouvons représenter l'image comme un vecteur de feature.
Les méthodes d'extraction sont ORB et AKAZE.

** KMeans
OpenCV implémente aussi l'algorithme KMeans. Celui-ci permet de regrouper les
objets similaires en fonction de leur feature. Dans notre cas, il va nous
permettre de créer 2 groupes d'images : logo / non logo.
L'avantage de KMeans est qu'il est est très rapide et assez efficace dans la
plupart des cas. C'est l'un des algorithmes de clusterisation les plus utilisés.


** Expérimentation et résultat:
Ensemble de test : une vidéo de ligue 1, une vidéo de liga, une vidéo de
premier league et une vidéo NFL.
Dans toutes les expérimentations, la vidéo est découpée en shot (plan).
Soit S l'ensemble des shots.

** 1 frame par shot
[[file:orb_simple_res.JPG]]
- Récupérer le frame à la fin de chaque shot
  - nous obtenons |S| frame
- Pour chaque frame, calculer ses features (orb ou akaze)
  - Nous obtenons |S| vecteurs
- Utiliser KMeans avec K=2 pour séparer les vecteurs en deux groupes
  - le groupe le plus petit est le groupe des logo

Résultats :
*** TODO meilleurs res
Mauvais sur toutes les vidéos

** W frames par shot:
- Récupérer W frames pour chaque shot
  - nous obtenons |S*W| frame, où W est le nombre de frame
- Pour chaque frame, calculer ses features (orb ou akaze)
  - nous obtenons |S*W| vecteurs
- Utiliser KMeans avec K=2 pour séparer les vecteurs en deux groupes
  - le groupe le plus petit est le groupe des logo

Résultats :
Mauvais sur toutes les vidéos

** 1 fenêtre de frame par shot:
[[file:akaze_window_res.JPG]]
- Récupérer W frames pour chaque shot, les régrouper en une fenêtre
  - nous obtenons |S| fenêtre de dimension W, où W est le nombre de frame
- Pour chaque fenêtre, calculer ses features (orb ou akaze)
  - Nous obtenons un vecteur de dimension |S*W|
- Utiliser KMeans avec K=2 pour séparer les vecteurs en deux groupes
  - le groupe le plus petit est le groupe des logo

Résultats:
De bons résultats sur la vidéo de PL.
Mauvais résultats sur les autres vidéos.


** 1 fenêtre de frame par shot et différence des frames dans la fenêtre:
[[file:orb_window_diff_res.JPG]]
- Récupérer W frames pour chaque shot, les régrouper en une fenêtre
  - nous obtenons |S| fenêtre de dimension W, où W est le nombre de frame
- Pour chaque fenêtre, calculer la matrice M égale à la différence de toute
  les autres frames dans la fenêtre
- Pour chaque matrice de différence, calculer ses features
  - nous obtenons |S| vecteur s
- Utiliser KMeans avec K=2 pour séparer les vecteurs en deux groupes
  - le groupe le plus petit est le groupe des logo

Résultats :
De bons résultats sur la vidéo de PL.
Mauvais résultats sur les autres vidéos.


* Seconde approche : matching de contours
La méthode choisie différe avec les autres sur un point : au lieu de chercher
à différencier les frames logo des frames non-logo, nous allons chercher
les frames qui ont des formes en commun dans la vidéo.
En effet, d'après l'hypothèse III, il est fort probable que si un frame à
l'instant t a beaucoup de formes en commun avec un frame à l'instant t', avec
2 < t' < 90 (hypothèse V), alors il y a un logo à l'instant t et un logo à
l'instant t', et un replay entre t et '.
** Algorithme
- Pré traitement sur les shots
  1. Redimensionner
  2. Cropper
  3. Supprimer le background (s’étendre la dessus)
  4. Détecter le contour (Canny Edge Detection)
  5. Génération des mosaiques TODO : explain this
- Pour chaque mosaique de plan S_A :
  - Pour chaque mosaique de plan S_B après S_A :
    1. Contour_commun = C_A & C_B
    2. Contours_diff = Détection du contour de Contour_commun (cv2.findContours)
    3. Résultat = Ne garder que les contours qui sont assez longs (ceux qui ont au moins K points)
    4. Si Résultat > Seuil : alors S_A et S_B sont des logos potentiels
- Pour chaque logo potentiel LP :
  1. Le comparer avec les autres logo L’ (même procédure qu’en 2)
  2. Si au moins 2 logo L’  match, alors LP est un logo
- Trouver les replays grâce aux logos

Pré traitement :
Les frames sont resizé puis cropé vers le centre (pour ne pas avoir l'affichage
en haut de l'écran etc...), puis un blur est appliqué (bilateralFilter, permet
de filtrer certains faux positifs), et enfin on applique Canny Edge Detection.

Le point 3 de l’algorithme sert à filtrer les éventuels faux positifs.
Notre algorithme est sensible au plan fixe et aux images avec beaucoup de bruits
(ces images ont beaucoup de contours détectés par l’algorithme de détection de contours).
Beaucoup de ces faux-positifs peuvent être filtrer lors du pré-traitement sur les plans,
 notamment en rajoutant du blur ou en supprimant le background, néanmoins, nous ne sommes
pas parvenus à filtrer 100% des faux-positifs.

** Mosaique de plan
*** TODO resize img
[[file:mosaique1.png]]
[[file:mosaique2.png]]
Pour chaque shot deux images au format .png (pas au format jpg, car celui-ci prend trop d'espace disque) sont générées.

Chaque image est de dimension I * I * width * height où I est le nombre de frame dans le shot.

Ces images sont en faites des matrices d'image qui vont permettre de comparer rapidement deux shot.
La première matrice a un décalage d'un frame par ligne, la seconde n'a pas de décalage.

Pour comparer deux shot, il suffit d’appliquer un ET binaire entre les matrices des mosaiques,
puis de calculer la longueur du conteur dans cette matrice.


[[file:mosaique3.png]]

** Résultats et limitation
Les résultats sans le filtrage des faux positifs (l’étape 3 de l’algorithme) sont un
bon moyen d’évaluer l’efficacité de notre méthode.
*** TODO : mettre les résultats ici
Concernant le temps d’exécution, celui-ci est relié presque entièrement à la taille de
la vidéo donnée en entrée, ainsi qu’à la taille des mosaiques.

Les limitations de notre méthode sont les suivantes :
- Dans certaines vidéos, il n’y a pas de logo pour les replays (simple fondu)
- Dans certaines vidéos, les logo de début et fin de replay ne sont pas les mêmes.
- Dans certains vidéos, il y a des logo au début des replays, mais pas de logo à
  la fin des replays (un simple fondu remplace le logo).


* Apprentissage profond : état de l'art pour la reconnaissance d'action dans les vidéos
Nous nous intéressons à l'état de l'art concernant la détection d'action
dans les vidéos. En effet, la transition d'un logo s'effectue sur plusieurs
frames consécutives; il y a donc une composante temporelle à notre recherche,
et nous pouvons considérer la transition d'un logo comme une action.

** Two-Stream Convolutional Networks for Action Recognition in Videos
Cet article est écrit par Karen Simonyan et Andrew Zisserman. Dans celui-ci,
ils proposent de séparer la tâche de reconnaissance d'action dans les vidéos en
deux parties : une composante spatiale et une composante temporelle.
La composante spatiale contient l'information concernant sur les objets dans
la vidéo; tandis que la composante temporelle l'information sur les
déplacements de ces objets et de la caméra.
A partir de ces observations, les auteurs proposent d'entraîner un classifieur spatial (Spatial stream ConvNet) et un classifieur temporel (Temporal stream ConvNet).
Ces classifieurs sont des réseaux de neurones convolutifs profonds.

*** Classifieur spatial
Ce réseau a une architecture de classifieur d'image classique. Il va permettre
de donner un indice fort pour la prédiction, car certaines actions sont très
liées à certains objets.
De plus, la recherche dans le domaine de la classification est un domaine à part entière;
toutes les avancées dans le domaine augmenteront l'efficacité de ce classifieur.
Il n'est pas nécessaire d'apprendre ce réseau "from scratch" (de zéro), les approches
de transfer learning sont efficaces.

*** Classifieur temporel
[[file:optical_flow.png]]
Source : Two-Stream Convolutional Networks for Action Recognition in Videos, Figure 2
L'innovation de l'article vient de l'introduction du classifieur temporel.
L'idée est de détecter le mouvement des objets dans la vidéo, car un mouvement est la représentation d'un objet dans le temps.
Les auteurs appellent leur approche "optical flow stacking" (empilement de flux optique).
Dans celle-ci, ils utilisent la méthode "optical flow" pour détecter le mouvement des objets entre des frames consécutives.
Ils définissent aussi un hyperparamètre L qui définit la distance maximum entre deux frames
pour laquel il faut calculer le flux optique. Par exemple, si L=5, alors pour le
frame t, il faudra calculer le flux entre le frame t et le frame t+1; entre
t+1 et t+2; etc... jusqu'à t+4 à t+5. Chacun de ces flux servira d'entrée au classifieur temporel pour le frame t.

*** Méthode d'évalutation et résultats obtenus
[[file:two_stream_res.png]]
Source : Table 4: Mean accuracy (over three splits) on UCF-101 and HMDB-51.

Le classifieur spatial est pré-entrainé avec ImageNet, tandis que le temporel
est entraîné de zéro (car il n'y a pas de réseau déjà entraîné pour cette tâche).
Les dataset utilisés pour l'entraînement et l'évaluation sont UCF-101 et HMDB-51,
contenant à eux deux près de 20000 vidéos annotées.

*Note* Pour calculer la classe d'un frame à l'instant t, les auteurs proposent
deux méthodes :
- fusion par la moyenne (by averaging) : y_t = y_t_spatial + y_t_temporal / 2
- fusion par SVM (by SVM) : un SVM multiclasse linéaire est entrainé pour
prédire la classe à partir du softmax des scores L2-normalisés.

Les résultats en *link table* montrent l'efficacité de leur méthode par rapport
aux autres approches état de l'art.

Nous pouvons voir que leur approche two-stream avec fusion SVM est la plus
efficace sur le dataset UCF-101, et qu'elle a aussi de bons résultats sur
HMDB-51.

Ce qui est le plus intéressant dans cet article, c'est l'amélioration
qu'apporte l'ajout de la composante temporelle.
En effet, le classifieur d'image simple (spatial) n'a que 73.0% (UCF-101) et 40.5% (HMBD-51), tandis que le classifieur qui prend en compte l'image et la temporalité (two-stream model) atteint *88.0%* et 59.4%; ce qui est une nette amélioration.
Cet article nous a renforcé dans l'hypothèse qu'il est nécessaire d'étudier une vidéo non pas comme une suite d'image indépendante, mais comme une suite de séquence avec un lien au sein de chaque séquence.
Il semblerai que la temporalité a une très grande importance pour  l'analyse de vidéos.

** Learning Spatiotemporal Features with 3D Convolutional Networks
Dans cet article, Du Tran et. al proposent une approche pour apprendre les caractéristiques spatio-temporelles dans les vidéos grâce à un réseau de neuronnes profond.
L'objectif est d'apprendre des caractéristiques qui soient :
- générique : c'est à dire la capacité à representer différents types de vidéos
- compact : afin de pouvoir stocker un grand nombre de ces caractéristiques
- efficace (computationnellement): pour traiter les vidéos en temps réel
- simple (à implémenter) : afin de fonctionner même avec les modèles simples (comme un classifieur linéaire)

*** Apport de l'article
[[file:3D_vs_2_stream.JPG]]
Source : Learning Spatiotemporal Features with 3D Convolutional Networks Figure 1
Dans l'approche Two-Stream Convolutional Networks for Action Recognition in Videos, deux réseaux à convolution 2D sont utilisés (un spatial et un temporel).
L'apprentissage du réseau temporel est séparé de celui du réseau réseau spatial, ce qui ne permet pas d'avoir un réseau capturant l'information temporel relativement à l'information spatial.
De plus, même si le réseau temporel prend plusieurs frames en entrée (paramètre L > 1), la convolution 2D va écraser la composante temporelle car sa sortie sera une image (2D).
La figure *XXX* illustre la différence entre un réseau à convolution 2D (approche de K. Simonyan et A. Zisserman) et un réseau à convolution 3D (Tran et al.). Un réseau à convolution 2D aura en sortie une image (2D) même si l'entrée est une séquence d'images, tandis qu'un réseau à convolution 3D aura en sortie une image relative à une autre dimension (dans notre cas, le temps).

En choisissant d'entraîner leur réseau à partir de séquence d'images, les auteurs espèrent pouvoir apprendre la temporalité d'une manière "moins artificielle" que dans l'approche précédente (qui nécessite un pré-traitement sur les images pour pouvoir calculer les images d'optical flow servant au réseau temporel).

*** Architecture et entraînement du réseau
[[file:c3d_architecure.jpg]]
Source : Figure 3
L'entrée de ce réseau est de dimension c * l * h * w où c est le nombre de canal des images (3 pour la couleur, 1 pour les images en noir et blanc), l le nombre d'image dans les séquences, h la longueur et w la largeur.
L'architecture conseillée par les auteurs est 8 couches de convolution et 5 couches de pooling, ainsi que 2 couches complètement connectées et la fonction softmax pour la couche de sortie.
Le kernel recommandé par les auteurs est 3 * 3 * 3 avec un pas (stride) de 1 * 1 * 1 pour toutes les couches de convolution.
Toutes les couches de pooling sont max pooling avec une taille de kernel 2 * 2 * 2 (sauf pour la première qui est 1 * 2 * 2) avec un stride 2 * 2 * 2 (sauf pour la première qui a un stride de 1 * 2 * 2).
Pour finir avec l'architecture, les deux couches complètement connectées ont 4096 sorties.

Ce réseau va être entraîné de zéro par descente du gradient à partir de séquences d'images annotées.
Le taux d'apprentissage est de 0.003 et est divisé par 10 toutes les 4 epoch.
L'entraînement s'arrête après 16 epoch.

Après l'entraînement, le réseau peut être utilisé comme un extracteur de caractéristique pour des tâches d'analyse vidéo.
Pour se faire, la vidéo va être découpée en des clips de 16 frames (avec 8 frames de chevauchement entre deux clips consécutifs).
Ensuite, chacun de ces clips va être passé au réseau et l'avant dernière couche complètement connectée (fc6) va contenir les caractéristisques du clip.

* Qu'est-ce que ce réseau apprend ? * Ce réseau apprend à se focalisant sur l'image des premiers frames, et à traquer leur déplacement dans les frames suivants.

*** Résultat pour la tâche de reconnaissance d'action
[[file:c3d_result.jpg]]
Ces résultats ont été obtenus par les auteurs pour la tâche de reconnaissance d'action sur le corpus de vidéo UCF101.
Nous voyons que l'approche par réseau à convolution 3D est la plus efficace.

*** Conclusion
Dans cet article, les auteurs ont adressé le problème de la temporalité dans les vidéos.
Ils ont montré qu'un réseau à convolution 3D est capable de modéliser l'information temporelle et spatiale simultanément, et donc d'obtenir de meilleurs résultats que les réseaux à convolution 2D sur plusieurs tâches d'analyse de vidéos.
Pour ma part, je trouve cette approche très intéressante et élégante, de par sa simplicité.
En effet, le réseau apprend la temporalité sans qu'il soit nécessaire d'injecter l'information temporelle de manière artificielle ou bien en compliquant l'architecture du réseau.

** Beyond Short Snippets: Deep Networks for Video Classification
Dans cet article, les auteurs proposent d'utiliser une architecture hybride à base de CNN et de RNN (LSTM) pour l'analyse vidéo.
Les CNN sont des réseaux particulièrement efficaces pour analyser les frames des vidéos, c'est le CNN qui va se charger de la composante spatiale de la vidéo.

[[file:cnn_lstm.PNG]]
Source : Beyond Short Snippets: Deep Networks for Video Classification; Figure 3
* copié collé ce que j'ai fait pour mariage RNN*
Les LSTM sont un type particulier de RNN capables d'apprendre des dépendances à long terme.
Ceci est dû à leur capacité à stocker l'information dans leurs "memory cells".
Ils sont donc adaptés à l'apprentissage de la composante temporelle des vidéos.

Les auteurs proposent également d'utiliser d'incorporer l'information de mouvement des objets dans la vidéo en calculant le flux optique (comme dans la méthode Two-Stream Convolutional Networks for Action Recognition in Videos) des frames adjacents.

*** Approche
L'aspect le plus important de la recherche des auteurs concernent la manière dont le LSTM va recevoir l'information du CNN.
Deux architectures de CNN sont utilisés par les auteurs : AlexNet et GoogLeNet.
Afin que le LSTM puisse utiliser la sortie du CNN, il est nécessaire d'avoir une architecture de pooling efficace.
Les auteurs en proposent cinq.

* Collecte des données pour l'apprentissage profond
L'approche par matching de contours convient tout à fait à la tâche de scrapping.
En effet, elle est :
- rapide : moins de *X* minutes sur une machine *machine de référence EC2 (ou autre)* pour une vidéo de 90 minutes à 60 fps
- précise : seulement *X* % de faux positifs sur *Y* logos scrappés

*** Architecture du scrapper
- requête HTTP avec une ID youtube => logo uploadé sur GCP
- image docker (avec le serveur à l'écoute des requêtes) déployée sur le cloud
Cette architecture est scalaire; ceci nous a permi de scrapper plusieurs vidéos en parallèle et d'obtenir un dataset conséquent.

** Datasets
- Dataset non logo
- Dataset logo
- Dataset logo séparé en fonction du logo (ligue 1, premier league, ...)


** Détection des frames logo
*** Propre modèle
*** VGG net
*** Transfert Learning
*** Comparaison résultat

** Détection des séquences de frames logo

* Appendice
Clustering :
Histogramme :
Frame :
Shot : plan
FPS : frame per second / image par seconde
Cropper :
CNN : Convolutional Neural Network
LSTM : Long Short Term Memory

* Source
** Articles relatifs à l'apprentissage profond
Gradient-Based Learning Applied to Document Recognition; Y. LeCun, L. Bottou, Y. Bengio, P. Haffner; 1998
Learning Hierarchical Features for Scene Labeling; C. Farabet, C. Couprie, L. Y. LeCun; 2013
Two-Stream Convolutional Networks for Action Recognition in Videos; K. Simonyan, A. Zisserman; 2014
Learning Spatiotemporal Features with 3D Convolutional Networks; D. Tran, L. Bourdev; R. Fergus; L. Torresani; M. Paluri; 2014
Beyond Short Snippets: Deep Networks for Video Classification; J.Y.H. Ng, M. Hausknecht; 2015

** Articles relatifs à la détection de replays
A Robust Replay Detection Algorithm for Soccer Video; W. Xu, Y. Yi(2011).
Replay and key-events detection for sports video summarization using confined elliptical local ternary patterns and extreme learning machine; A. Javed, A. Irtaza; 2019
Video Co-summarization: Video Summarization by Visual Co-occurrence; W. Chu; 2015
Automatic Detection Of Replay Segments In Broadcast Sports Programs By Detection Of Logos In Scene Transitions; H. Pan; 2002
Mean Shift Based Video Segment Representation And Applications To Replay Detection; L. Duan; 2004
Online, Simultaneous Shot Boundary Detection And Key Frame Extraction For Sports Videos Using Rank Tracing; W. Abd-Almageed; ‎2008
On-line Key Frame Extraction and Video Boundary Detection using Mixed Scales Wavelets and SVD; A. Azeroual; 2016
Highlight Summarization in Soccer Video based on Goalmouth Detection; Z. Zhao; 2006
A General Framework for Automatic On-line Replay Detection in Sports Video; B. Han, Y. Yan; 2009
An Efficient Framework for Automatic Highlights Generation from Sports Videos; A. Javed; 2016
A New Slow-Motion Replay Extractor For Soccer Game Videos; E. Farn, L. Chen; 2003
Fast Highlight Detection and Scoring for Broadcast Soccer Video Summarization using On-Demand Feature Extraction and Fuzzy Inference; M. Sigari, H. Soltanian-Zadeh;  2015
Automatic Summarization Of Soccer Highlights Using Audio‑visual Descriptors; A. Raventós, R. Quijada, L. Torres, F. Tarrés; 2014
A Novel Method For Slow Motion Replay Detection In Broadcast Basketball Video; C. Chen; 2014

** Articles générals de vision par ordinateur
ORB: an efficient alternative to SIFT or SURF; E. Rublee, V. Rabaud